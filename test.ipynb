{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f14c6-0358-448b-9d7b-85385a7fd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pace: Plan\n",
    "## Step 1. Imports\n",
    "# Import packages\n",
    "# Import packages\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# For displaying all of the columns in dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "# For data modeling\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# For metrics and helpful functions\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
    "f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.tree import plot_tree\n",
    "# For saving models\n",
    "import pickle\n",
    "# Load dataset\n",
    "# Load dataset into a dataframe \n",
    "df0 = pd.read_csv(\"HR_capstone_dataset.csv\")\n",
    "# Display first few rows of the dataframe\n",
    "df0.head()\n",
    "## Step 2. Data Exploration (Initial EDA and data cleaning)\n",
    "# Gather basic information about the data\n",
    "# Gather basic information about the data \n",
    "df0.info()\n",
    "# Gather descriptive statistics about the data\n",
    "df0.describe()\n",
    "# Rename columns\n",
    "# Display all column names\n",
    "df0.columns\n",
    "# Rename columns as needed\n",
    "df0 = df0.rename(columns={'Work_accident': 'work_accident',\n",
    "                          'average_montly_hours': 'average_monthly_hours',\n",
    "                          'time_spend_company': 'tenure',\n",
    "                          'Department': 'department'})\n",
    "# Display all column names after the update\n",
    "df0.columns\n",
    "# check missing values\n",
    "# Check for missing values\n",
    "df0.isna().sum()\n",
    "# Check duplicates\n",
    "# Check for duplicates\n",
    "df0.duplicated().sum()\n",
    "# Inspect some rows containing duplicates as needed\n",
    "df0[df0.duplicated()].head()\n",
    "# Drop duplicates and save resulting dataframe in a new variable as needed\n",
    "df1 = df0.drop_duplicates(keep='first')\n",
    "# Display first few rows of new dataframe as needed\n",
    "df1.head()\n",
    "# Check outliers\n",
    "# Create a boxplot to visualize distribution of `tenure` and detect any outliers\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title('Boxplot to detect outliers for tenure', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "sns.boxplot(x=df1['tenure'])\n",
    "plt.show()\n",
    "# Determine the number of rows containing outliers \n",
    "# Compute the 25th percentile value in `tenure`\n",
    "percentile25 = df1['tenure'].quantile(0.25)\n",
    "# Compute the 75th percentile value in `tenure`\n",
    "percentile75 = df1['tenure'].quantile(0.75)\n",
    "# Compute the interquartile range in `tenure`\n",
    "iqr = percentile75 - percentile25\n",
    "# Define the upper limit and lower limit for non-outlier values in `tenure`\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"Lower limit:\", lower_limit)\n",
    "print(\"Upper limit:\", upper_limit)\n",
    "# Identify subset of data containing outliers in `tenure`\n",
    "outliers = df1[(df1['tenure'] > upper_limit) | (df1['tenure'] < lower_limit)]\n",
    "# Count how many rows in the data contain outliers in `tenure`\n",
    "print(\"Number of rows in the data containing outliers in `tenure`:\", len(outliers))\n",
    "# pAce: Analyze Stage\n",
    "## Step 2. Data Exploration (Continue EDA)\n",
    "# Get numbers of people who left vs. stayed\n",
    "print(df1['left'].value_counts())\n",
    "print()\n",
    "# Get percentages of people who left vs. stayed\n",
    "print(df1['left'].value_counts(normalize=True))\n",
    "# Data visualizations\n",
    "# Create a plot as needed \n",
    "# Set figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
    "# Create boxplot showing `average_monthly_hours` distributions for `number_project`, comparing employees who stayed versus those who left\n",
    "sns.boxplot(data=df1, x='average_monthly_hours', y='number_project', hue='left', orient=\"h\", ax=ax[0])\n",
    "ax[0].invert_yaxis()\n",
    "ax[0].set_title('Monthly hours by number of projects', fontsize='14')\n",
    "# Create histogram showing distribution of `number_project`, comparing employees who stayed versus those who left\n",
    "tenure_stay = df1[df1['left']==0]['number_project']\n",
    "tenure_left = df1[df1['left']==1]['number_project']\n",
    "sns.histplot(data=df1, x='number_project', hue='left', multiple='dodge', shrink=2, ax=ax[1])\n",
    "ax[1].set_title('Number of projects histogram', fontsize='14')\n",
    "# Display the plots\n",
    "plt.show()\n",
    "# Get value counts of stayed/left for employees with 7 projects\n",
    "df1[df1['number_project']==7]['left'].value_counts()\n",
    "# Create a plot as needed \n",
    "# Create scatterplot of `average_monthly_hours` versus `satisfaction_level`, comparing employees who stayed versus those who left\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.scatterplot(data=df1, x='average_monthly_hours', y='satisfaction_level', hue='left', alpha=0.4)\n",
    "plt.axvline(x=166.67, color='#ff6361', label='166.67 hrs./mo.', ls='--')\n",
    "plt.legend(labels=['166.67 hrs./mo.', 'left', 'stayed'])\n",
    "plt.title('Monthly hours by last evaluation score', fontsize='14');\n",
    "# Create a plot as needed \n",
    "# Set figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
    "# Create boxplot showing distributions of `satisfaction_level` by tenure, comparing employees who stayed versus those who left\n",
    "sns.boxplot(data=df1, x='satisfaction_level', y='tenure', hue='left', orient=\"h\", ax=ax[0])\n",
    "ax[0].invert_yaxis()\n",
    "ax[0].set_title('Satisfaction by tenure', fontsize='14')\n",
    "# Create histogram showing distribution of `tenure`, comparing employees who stayed versus those who left\n",
    "tenure_stay = df1[df1['left']==0]['tenure']\n",
    "tenure_left = df1[df1['left']==1]['tenure']\n",
    "sns.histplot(data=df1, x='tenure', hue='left', multiple='dodge', shrink=5, ax=ax[1])\n",
    "ax[1].set_title('Tenure histogram', fontsize='14')\n",
    "plt.show();\n",
    "# Calculate mean and median satisfaction scores of employees who left and those who stayed\n",
    "df1.groupby(['left'])['satisfaction_level'].agg([np.mean,np.median])\n",
    "# Create a plot as needed \n",
    "# Set figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
    "# Define short-tenured employees\n",
    "tenure_short = df1[df1['tenure'] < 7]\n",
    "# Define long-tenured employees\n",
    "tenure_long = df1[df1['tenure'] > 6]\n",
    "# Plot short-tenured histogram\n",
    "sns.histplot(data=tenure_short, x='tenure', hue='salary', discrete=1, \n",
    "             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.5, ax=ax[0])\n",
    "ax[0].set_title('Salary histogram by tenure: short-tenured people', fontsize='14')\n",
    "# Plot long-tenured histogram\n",
    "sns.histplot(data=tenure_long, x='tenure', hue='salary', discrete=1, \n",
    "             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.4, ax=ax[1])\n",
    "ax[1].set_title('Salary histogram by tenure: long-tenured people', fontsize='14');\n",
    "# Create a plot as needed \n",
    "# Create scatterplot of `average_monthly_hours` versus `last_evaluation`\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.scatterplot(data=df1, x='average_monthly_hours', y='last_evaluation', hue='left', alpha=0.4)\n",
    "plt.axvline(x=166.67, color='#ff6361', label='166.67 hrs./mo.', ls='--')\n",
    "plt.legend(labels=['166.67 hrs./mo.', 'left', 'stayed'])\n",
    "plt.title('Monthly hours by last evaluation score', fontsize='14');\n",
    "# Create a plot as needed \n",
    "# Create plot to examine relationship between `average_monthly_hours` and `promotion_last_5years`\n",
    "plt.figure(figsize=(16, 3))\n",
    "sns.scatterplot(data=df1, x='average_monthly_hours', y='promotion_last_5years', hue='left', alpha=0.4)\n",
    "plt.axvline(x=166.67, color='#ff6361', ls='--')\n",
    "plt.legend(labels=['166.67 hrs./mo.', 'left', 'stayed'])\n",
    "plt.title('Monthly hours by promotion last 5 years', fontsize='14');\n",
    "# Display counts for each department\n",
    "df1[\"department\"].value_counts()\n",
    "# Create a plot as needed \n",
    "# Create stacked histogram to compare department distribution of employees who left to that of employees who didn't\n",
    "plt.figure(figsize=(11,8))\n",
    "sns.histplot(data=df1, x='department', hue='left', discrete=1, \n",
    "             hue_order=[0, 1], multiple='dodge', shrink=.5)\n",
    "plt.xticks(rotation='45')\n",
    "plt.title('Counts of stayed/left by department', fontsize=14);\n",
    "# Create a plot as needed \n",
    "# Plot a correlation heatmap\n",
    "plt.figure(figsize=(16, 9))\n",
    "heatmap = sns.heatmap(df0.corr(), vmin=-1, vmax=1, annot=True, cmap=sns.color_palette(\"vlag\", as_cmap=True))\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':14}, pad=12);\n",
    "# paCe: Construct Stage\n",
    "## Step 3. Model Building, Step 4. Results and Evaluation\n",
    "\n",
    "# Modeling Approach A: Logistic Regression Model\n",
    "# Logistic regression\n",
    "# Copy the dataframe\n",
    "df_enc = df1.copy()\n",
    "# Encode the `salary` column as an ordinal numeric category\n",
    "df_enc['salary'] = (\n",
    "    df_enc['salary'].astype('category')\n",
    "    .cat.set_categories(['low', 'medium', 'high'])\n",
    "    .cat.codes\n",
    ")\n",
    "# Dummy encode the `department` column\n",
    "df_enc = pd.get_dummies(df_enc, drop_first=False)\n",
    "# Display the new dataframe\n",
    "df_enc.head()\n",
    "# Create a heatmap to visualize how correlated variables are\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_enc[['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'tenure']]\n",
    "            .corr(), annot=True, cmap=\"crest\")\n",
    "plt.title('Heatmap of the dataset')\n",
    "plt.show()\n",
    "# Create a stacked bart plot to visualize number of employees across department, comparing those who left with those who didn't\n",
    "# In the legend, 0 (purple color) represents employees who did not leave, 1 (red color) represents employees who left\n",
    "pd.crosstab(df1['department'], df1['left']).plot(kind ='bar',color='mr')\n",
    "plt.title('Counts of employees who left versus stayed across department')\n",
    "plt.ylabel('Employee count')\n",
    "plt.xlabel('Department')\n",
    "plt.show()\n",
    "# Select rows without outliers in `tenure` and save resulting dataframe in a new variable\n",
    "df_logreg = df_enc[(df_enc['tenure'] >= lower_limit) & (df_enc['tenure'] <= upper_limit)]\n",
    "# Display first few rows of new dataframe\n",
    "df_logreg.head()\n",
    "# Isolate the outcome variable\n",
    "y = df_logreg['left']\n",
    "# Display first few rows of the outcome variable\n",
    "y.head() \n",
    "# Select the features you want to use in your model\n",
    "X = df_logreg.drop('left', axis=1)\n",
    "# Display the first few rows of the selected features \n",
    "X.head()\n",
    "# Split the data into training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "# Construct a logistic regression model and fit it to the training dataset\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=500).fit(X_train, y_train)\n",
    "# Use the logistic regression model to get predictions on the test set\n",
    "y_pred = log_clf.predict(X_test)\n",
    "# Compute values for confusion matrix\n",
    "log_cm = confusion_matrix(y_test, y_pred, labels=log_clf.classes_)\n",
    "# Create display of confusion matrix\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, \n",
    "                                  display_labels=log_clf.classes_)\n",
    "# Plot confusion matrix\n",
    "log_disp.plot(values_format='')\n",
    "# Display plot\n",
    "plt.show()\n",
    "df_logreg['left'].value_counts(normalize=True)\n",
    "# Create classification report for logistic regression model\n",
    "target_names = ['Predicted would not leave', 'Predicted would leave']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "# Create classification report for logistic regression model\n",
    "target_names = ['Predicted would not leave', 'Predicted would leave']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Modeling Approach B: Tree-based Model\n",
    "# Isolate the outcome variable\n",
    "y = df_enc['left']\n",
    "# Display the first few rows of `y`\n",
    "y.head()\n",
    "# Select the features\n",
    "X = df_enc.drop('left', axis=1)\n",
    "# Display the first few rows of `X`\n",
    "X.head()\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\n",
    "# Decision tree - Round 1\n",
    "# Instantiate model\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "# Assign a dictionary of hyperparameters to search over\n",
    "cv_params = {'max_depth':[4, 6, 8, None],\n",
    "             'min_samples_leaf': [2, 5, 1],\n",
    "             'min_samples_split': [2, 4, 6]\n",
    "             }\n",
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
    "# Instantiate GridSearch\n",
    "tree1 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='roc_auc')\n",
    "%%time\n",
    "tree1.fit(X_train, y_train)\n",
    "# Check best parameters\n",
    "tree1.best_params_\n",
    "# Check best AUC score on CV\n",
    "tree1.best_score_\n",
    "def make_results(model_name:str, model_object, metric:str):\n",
    "    '''\n",
    "    Arguments:\n",
    "        model_name (string): what you want the model to be called in the output table\n",
    "        model_object: a fit GridSearchCV object\n",
    "        metric (string): precision, recall, f1, accuracy, or auc\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, accuracy, and auc scores\n",
    "    for the model with the best mean 'metric' score across all validation folds.  \n",
    "    '''\n",
    "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
    "    metric_dict = {'auc': 'mean_test_roc_auc',\n",
    "                   'precision': 'mean_test_precision',\n",
    "                   'recall': 'mean_test_recall',\n",
    "                   'f1': 'mean_test_f1',\n",
    "                   'accuracy': 'mean_test_accuracy'\n",
    "                  }\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "    # Isolate the row of the df with the max(metric) score\n",
    "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
    "    # Extract Accuracy, precision, recall, and f1 score from that row\n",
    "    auc = best_estimator_results.mean_test_roc_auc\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision],\n",
    "                          'recall': [recall],\n",
    "                          'F1': [f1],\n",
    "                          'accuracy': [accuracy],\n",
    "                          'auc': [auc]\n",
    "                        })\n",
    "    return table\n",
    "# Random forest - Round 1\n",
    "# Get all CV scores\n",
    "tree1_cv_results = make_results('decision tree cv', tree1, 'auc')\n",
    "tree1_cv_results\n",
    "# Instantiate model\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "# Assign a dictionary of hyperparameters to search over\n",
    "cv_params = {'max_depth': [3,5, None], \n",
    "             'max_features': [1.0],\n",
    "             'max_samples': [0.7, 1.0],\n",
    "             'min_samples_leaf': [1,2,3],\n",
    "             'min_samples_split': [2,3,4],\n",
    "             'n_estimators': [300, 500],\n",
    "             }  \n",
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
    "# Instantiate GridSearch\n",
    "rf1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')\n",
    "%%time\n",
    "rf1.fit(X_train, y_train) # --> Wall time: ~10min\n",
    "# Define a path to the folder where you want to save the model\n",
    "path = '/home/jovyan/work/'\n",
    "def write_pickle(path, model_object, save_as:str):\n",
    "    '''\n",
    "    In: \n",
    "        path:         path of folder where you want to save the pickle\n",
    "        model_object: a model you want to pickle\n",
    "        save_as:      filename for how you want to save the model\n",
    "\n",
    "    Out: A call to pickle the model in the folder indicated\n",
    "    '''    \n",
    "\n",
    "    with open(path + save_as + '.pickle', 'wb') as to_write:\n",
    "        pickle.dump(model_object, to_write)\n",
    "def read_pickle(path, saved_model_name:str):\n",
    "    '''\n",
    "    In: \n",
    "        path:             path to folder where you want to read from\n",
    "        saved_model_name: filename of pickled model you want to read in\n",
    "\n",
    "    Out: \n",
    "        model: the pickled model \n",
    "    '''\n",
    "    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n",
    "        model = pickle.load(to_read)\n",
    "\n",
    "    return model\n",
    "# Write pickle\n",
    "write_pickle(path, rf1, 'hr_rf1')\n",
    "# Read pickle\n",
    "rf1 = read_pickle(path, 'hr_rf1')\n",
    "# Check best AUC score on CV\n",
    "rf1.best_score_\n",
    "# Check best params\n",
    "rf1.best_params_\n",
    "# Get all CV scores\n",
    "rf1_cv_results = make_results('random forest cv', rf1, 'auc')\n",
    "print(tree1_cv_results)\n",
    "print(rf1_cv_results)\n",
    "def get_scores(model_name:str, model, X_test_data, y_test_data):\n",
    "    '''\n",
    "    Generate a table of test scores.\n",
    "    In: \n",
    "        model_name (string):  How you want your model to be named in the output table\n",
    "        model:                A fit GridSearchCV object\n",
    "        X_test_data:          numpy array of X_test data\n",
    "        y_test_data:          numpy array of y_test data\n",
    "    Out: pandas df of precision, recall, f1, accuracy, and AUC scores for your model\n",
    "    '''\n",
    "    preds = model.best_estimator_.predict(X_test_data)\n",
    "    auc = roc_auc_score(y_test_data, preds)\n",
    "    accuracy = accuracy_score(y_test_data, preds)\n",
    "    precision = precision_score(y_test_data, preds)\n",
    "    recall = recall_score(y_test_data, preds)\n",
    "    f1 = f1_score(y_test_data, preds)\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision], \n",
    "                          'recall': [recall],\n",
    "                          'f1': [f1],\n",
    "                          'accuracy': [accuracy],\n",
    "                          'AUC': [auc]\n",
    "                         })\n",
    "    return table\n",
    "# Get predictions on test data\n",
    "rf1_test_scores = get_scores('random forest1 test', rf1, X_test, y_test)\n",
    "rf1_test_scores\n",
    "# Feature Engineering\n",
    "# Drop `satisfaction_level` and save resulting dataframe in new variable\n",
    "df2 = df_enc.drop('satisfaction_level', axis=1)\n",
    "# Display first few rows of new dataframe\n",
    "df2.head()\n",
    "# Create `overworked` column. For now, it's identical to average monthly hours.\n",
    "df2['overworked'] = df2['average_monthly_hours']\n",
    "# Inspect max and min average monthly hours values\n",
    "print('Max hours:', df2['overworked'].max())\n",
    "print('Min hours:', df2['overworked'].min())\n",
    "# Define `overworked` as working > 175 hrs/week\n",
    "df2['overworked'] = (df2['overworked'] > 175).astype(int)\n",
    "# Display first few rows of new column\n",
    "df2['overworked'].head()\n",
    "# Drop the `average_monthly_hours` column\n",
    "df2 = df2.drop('average_monthly_hours', axis=1)\n",
    "# Display first few rows of resulting dataframe\n",
    "df2.head()\n",
    "# Isolate the outcome variable\n",
    "y = df2['left']\n",
    "# Select the features\n",
    "X = df2.drop('left', axis=1)\n",
    "# Create test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\n",
    "# Decision tree - Round 2\n",
    "# Instantiate model\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "# Assign a dictionary of hyperparameters to search over\n",
    "cv_params = {'max_depth':[4, 6, 8, None],\n",
    "             'min_samples_leaf': [2, 5, 1],\n",
    "             'min_samples_split': [2, 4, 6]\n",
    "             }\n",
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
    "# Instantiate GridSearch\n",
    "tree2 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='roc_auc')\n",
    "%%time\n",
    "tree2.fit(X_train, y_train)\n",
    "# Check best params\n",
    "tree2.best_params_\n",
    "# Check best AUC score on CV\n",
    "tree2.best_score_\n",
    "# Get all CV scores\n",
    "tree2_cv_results = make_results('decision tree2 cv', tree2, 'auc')\n",
    "print(tree1_cv_results)\n",
    "print(tree2_cv_results)\n",
    "# Random forest - Round 2\n",
    "# Instantiate model\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "# Assign a dictionary of hyperparameters to search over\n",
    "cv_params = {'max_depth': [3,5, None], \n",
    "             'max_features': [1.0],\n",
    "             'max_samples': [0.7, 1.0],\n",
    "             'min_samples_leaf': [1,2,3],\n",
    "             'min_samples_split': [2,3,4],\n",
    "             'n_estimators': [300, 500],\n",
    "             }  \n",
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
    "# Instantiate GridSearch\n",
    "rf2 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')\n",
    "%%time\n",
    "rf2.fit(X_train, y_train) # --> Wall time: 7min 5s\n",
    "# Write pickle\n",
    "write_pickle(path, rf2, 'hr_rf2')\n",
    "# Read in pickle\n",
    "rf2 = read_pickle(path, 'hr_rf2')\n",
    "# Check best params\n",
    "rf2.best_params_\n",
    "# Check best AUC score on CV\n",
    "rf2.best_score_\n",
    "# Get all CV scores\n",
    "rf2_cv_results = make_results('random forest2 cv', rf2, 'auc')\n",
    "print(tree2_cv_results)\n",
    "print(rf2_cv_results)\n",
    "# Get predictions on test data\n",
    "rf2_test_scores = get_scores('random forest2 test', rf2, X_test, y_test)\n",
    "rf2_test_scores\n",
    "# Generate array of values for confusion matrix\n",
    "preds = rf2.best_estimator_.predict(X_test)\n",
    "cm = confusion_matrix(y_test, preds, labels=rf2.classes_)\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=rf2.classes_)\n",
    "disp.plot(values_format='');\n",
    "# Decision tree splits\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(85,20))\n",
    "plot_tree(tree2.best_estimator_, max_depth=6, fontsize=14, feature_names=X.columns, \n",
    "          class_names={0:'stayed', 1:'left'}, filled=True);\n",
    "plt.show()\n",
    "# Decision tree feature importance\n",
    "#tree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, columns=X.columns)\n",
    "tree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, \n",
    "                                 columns=['gini_importance'], \n",
    "                                 index=X.columns\n",
    "                                )\n",
    "tree2_importances = tree2_importances.sort_values(by='gini_importance', ascending=False)\n",
    "# Only extract the features with importances > 0\n",
    "tree2_importances = tree2_importances[tree2_importances['gini_importance'] != 0]\n",
    "tree2_importances\n",
    "sns.barplot(data=tree2_importances, x=\"gini_importance\", y=tree2_importances.index, orient='h')\n",
    "plt.title(\"Decision Tree: Feature Importances for Employee Leaving\", fontsize=12)\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "# Random forest feature importance\n",
    "# Get feature importances\n",
    "feat_impt = rf2.best_estimator_.feature_importances_\n",
    "# Get indices of top 10 features\n",
    "ind = np.argpartition(rf2.best_estimator_.feature_importances_, -10)[-10:]\n",
    "# Get column labels of top 10 features \n",
    "feat = X.columns[ind]\n",
    "# Filter `feat_impt` to consist of top 10 feature importances\n",
    "feat_impt = feat_impt[ind]\n",
    "y_df = pd.DataFrame({\"Feature\":feat,\"Importance\":feat_impt})\n",
    "y_sort_df = y_df.sort_values(\"Importance\")\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "y_sort_df.plot(kind='barh',ax=ax1,x=\"Feature\",y=\"Importance\")\n",
    "ax1.set_title(\"Random Forest: Feature Importances for Employee Leaving\", fontsize=12)\n",
    "ax1.set_ylabel(\"Feature\")\n",
    "ax1.set_xlabel(\"Importance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
